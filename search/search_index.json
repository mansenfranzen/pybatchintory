{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"developers/specification/implementation/","title":"Implementation","text":"<p>This document represents the implementation specification for a software project named <code>pybatchintory</code>. It focuses on how the requirement specification is going to be realized.</p>"},{"location":"developers/specification/requirements/","title":"Requirements","text":"<p>This document outlines the requirements specification for a software project named <code>pybatchintory</code>. It focuses on what the software aims to accomplish opposed to how it is achieved (see implementation specification). </p> <p>The problem statement is formulated and the added value is justified. Functional and non-functional requirements are derived. </p> <p>The intended audience are primarily developers, engineers and architects.</p>"},{"location":"developers/specification/requirements/#general-description","title":"General description","text":""},{"location":"developers/specification/requirements/#purpose","title":"Purpose","text":"<p><code>pybatchintory</code> represents a middleware for batch processing data pipelines. It reduces maintenance efforts, improves performance and enhances observability by providing fist class support for following features:</p> <ul> <li>Incremental processing: Process only new, unseen data assets without custom bookmarking logic.</li> <li>Backfilling: Reprocess historical data assets in a configurable and automated manner without manual intervention.</li> <li>Configurable workloads: Define the amount of data assets to be processed for best predictability and efficiency.</li> <li>Transparency: Enrich processed data assets with job details like timestamp, identifier and parametrization.</li> </ul> <p>Conceptionally, batch processing applications delegate the responsibility of generating batches of data assets to <code>pybatchintory</code> to leverage these features. <code>pybatchintory</code> provides an API to generate and interact with batches of data assets while maintaining state of historically processed data assets. Importantly, <code>pybatchintory</code> only consumes metadata about data assets while the actual data is not read.</p> <p>The primary use case are file-based batch processing pipelines. A typical real world example are Parquet or JSON files which are continuously added to a cloud object store like Azure Blob or AWS S3. Periodically, these files need to processed in batches via a distributed computation framework like Apache Spark or Dask.</p> <p>However, <code>pybatchintory</code> is not limited to any specific type or format because it is data asset agnostic. It is possible to generate batches of work that represent partitioning keys in table formats or distributed databases. </p>"},{"location":"developers/specification/requirements/#rationale","title":"Rationale","text":"<p>The following section summarizes the contextual background from which <code>pybatchintory</code> originates with its purpose and added value.  </p>"},{"location":"developers/specification/requirements/#incremental-processing","title":"Incremental processing","text":"<p>Incremental processing greatly improves data pipeline performance by only processing unseen data assets. This prevents costly reprocessing of entire datasets. </p> <p>While incremental processing is supported in stream processing frameworks by design (e.g. Flink or Kafka Streams), this functionality is rarely available in batch processing frameworks. Only very few off-the-shelf solutions provide easy-to-use abstractions to handle incremental batch semantics (e.g., AWS Glue Bookmarks). More often than not, custom solutions are used in production environments. Typically, these rely on timestamp comparisons or manually maintained offsets which closely resembles what stream processing frameworks offer out-of-the-box.</p> Why not use stream instead of batch processing? <p>For non-time-sensitive tasks, batch processing provides two significant advantages over stream processing:</p> <ol> <li> <p>Resource efficiency: Batch processing allows for aggregation of large amounts of data at once, utilizing compute and memory resources more effectively while being more cost-effective overall.</p> </li> <li> <p>Managing complexity: Batch processing is easer to reason about because data is bounded and finite. Moreover, implementing complex logic on unbounded streams of data is very challenging to develop and maintain.</p> </li> </ol> What other solutions exist for incremental batch processing? <p>More solutions exist which offer incremental processing. However, they tend fall short in one regard or the other.</p> <ul> <li>Databrick's Autoloader enables incremental processing of files in cloud objects stores. However, this does not support batch processing but only integrates with Spark Streaming and its associated limitations.</li> <li>Likewise, Flink's unbounded file data source continously monitors for new files at any given location. However, downstream computations using this data source can only run in unbounded, streaming execution mode which does not leverage the benefits of batch processing.</li> <li>Apache Hudi's DeltaStreamer allows to incrementally ingest data from various sources into Hudi tables while applying custom transformations on the raw data. While this approach supports batch processing, it only works for Hudi tables and is mainly intended for less complex ingestion use cases.</li> </ul>"},{"location":"developers/specification/requirements/#backfilling","title":"Backfilling","text":"<p>Reprocessing of data is not an uncommon theme. Bugs need to be fixed in production pipelines not just for future but also for existing past data. Added features such as new KPIs need to be made available not just from now on but also for the data of recent years. Hence, historical data needs to be reprocessed. </p> <p>However, first class support for such scenarios is currently not available. Processing all historical data at once can eventually become very costly and error prone because the accumulated volume of data may exceed a threshold of feasibility. Hence, manual planning and execution is often required to create processable chunks of work.</p>"},{"location":"developers/specification/requirements/#configurable-workloads","title":"Configurable Workloads","text":"<p>Data assets are not generated evenly over time. Rather, the data generation process often exhibits seasonality and spikes resulting in greatly differing volumes of data per time interval. If auto-scaling is not available, processing applications may drastically drop in performance (e.g., due to insufficient memory forcing disk spills) or may even fail completely (e.g., out of memory error). Even with auto-scaling being enabled, performance degradation is likely because perfect parallelization via horizontal scaling is almost never possible in distributed systems. </p> <p>A simple and effective solution is to limit the amount of data to be processed at once by a single batch job. However, batch processing frameworks don't provide native support this functionality, yet.</p>"},{"location":"developers/specification/requirements/#observability","title":"Observability","text":"<p>Frequently, doubts arise regarding data integrity and reliability. Customers may question results shown in an end user dashboard. To be able to troubleshoot, underlying data pipelines need to be easily inspectable to verify that all available data assets have been processed correctly. Moreover, a certain data asset needs to be traceable in regard to when it was processed by which processing application. </p>"},{"location":"developers/specification/requirements/#terminiology","title":"Terminiology","text":"<p>This section defines core concepts and provides a high level overview:</p> <p></p> <ul> <li>Processing application: Resembles the application that actually reads and processes the content of data assets. It utilizes <code>pybatchintory</code> to generate batches of data assets.</li> <li>Batch configuration: Specifies the characteristics of the batch generation process.</li> <li>Jobs: Represent one or more executions of the processing application. Each instantiation invokes <code>pybatchintory</code> to fetch batches of data assets.</li> <li>Batches: Contain references to one or more data assets.</li> <li>Batch generation: Resembles the process of generating new batches of data assets. Its result is determined by the batch configuration, the inventory table and the meta table. It embodies the core logic enabling incremental and backfilling semantics with predictable workloads. </li> <li>Inventory table: Represents the backend table of <code>pybatchintory</code>. It maintains state of historically executed jobs and processed data items.</li> <li>Meta table: Contains meta information about data assets like file location and size. <code>pybatchintory</code> leverages it to generate batches of data assets.</li> <li>Data assets: Represent arbitrary processable units of work. Typically, these are Parquet or JSON files stored in an object store such as AWS S3 or Azure Blob. However, since <code>pybatchintory</code> makes no strong assumptions about them, it can be anything consumable by a processing application.</li> </ul>"},{"location":"developers/specification/requirements/#precondition","title":"Precondition","text":"<p>In order to generate batches of data assets, the existence of a metadata table is mandatory. At minimum, it needs to contain a reference to the data item (e.g, file location) and a primary key as unique row identifier. Since pybatchintory generates chronologically sorted ranges of data items, the primary key needs to chronologically sorted, too.</p>"},{"location":"developers/specification/requirements/#limitation","title":"Limitation","text":"<p>The batch generation process will only support consecutive data assets in chronologically increasing order. This greatly reduces complextiy and simplifies the implementation. Future versions may loosen this restriction.</p>"},{"location":"developers/specification/requirements/#user-stories","title":"User Stories","text":"<ul> <li>As a developer, I want a dedicated solution to support incremental processing for batch pipelines.</li> <li>As a developer, I want a managed solution to reprocess historical data in configurable chunks.</li> <li>As a developer, I want to specify the amount of data that is consumed by a single processing application job.</li> <li>As a stakeholder/developer, I want predictable run times and best performance/cost-ratio.</li> <li>As a user/developer, I want to understand when a given data asset was processed by which job.</li> <li>As a user/developer, I want to retrieve the backlog size of unprocessed data assets for a given job.</li> </ul>"},{"location":"developers/specification/requirements/#specific-requirements","title":"Specific requirements","text":""},{"location":"developers/specification/requirements/#terminiology_1","title":"Terminiology","text":"<p>This section extends the existing terminiology with more detailed concepts which are relevant for the specific requirements.</p> <ul> <li>Backlog: Describes the remaining amount of unprocessed data assets for a given job.</li> <li>Data asset type: Summarizes data assets that semantically belong together such as a directory of parquet files containing IoT data accross various devices and time ranges.</li> <li>Batch window: Specifies the range of data assets to be included in the batch generation process. Data assets outside this window are not relevant for the batch generation process. The window is defined by a lower and upper boundary which define the first and last data assets to be included, respectively.</li> <li>Batch status: Defines the possible states of a batch of data assets. This includes \"processing\", \"succeeded\" and \"failed\".</li> </ul>"},{"location":"developers/specification/requirements/#functional-requirements","title":"Functional requirements","text":""},{"location":"developers/specification/requirements/#data-asset-selection","title":"Data Asset Selection","text":"<p>Allow defining the metadata table that references the relevant data assets.</p>"},{"location":"developers/specification/requirements/#data-asset-filtering","title":"Data Asset Filtering","text":"<p>Allow providing a filter condition to subset data assets from the metadata table. This is crucial in case the metadata table holds references to multiple data asset types.  The filtering method should be as flexible as possible to support a wide range of metadata table models.</p>"},{"location":"developers/specification/requirements/#incremental-processing_1","title":"Incremental processing","text":"<p>Allow fetching non-processed data assets with no upper boundary for the batch window. Hence, newly added data assets will be included from one job run to another. The total amount of relevant data assets is dynamic and changes over time.</p>"},{"location":"developers/specification/requirements/#backfilling_1","title":"Backfilling","text":"<p>Allow fetching non-processed data assets with an upper boundary for the batch window. Hence, newly added data assets will be excluded from one job run to another. The total amount of relevant data assets is fixed and remains constant over time.</p>"},{"location":"developers/specification/requirements/#chronology","title":"Chronology","text":"<p>Always fetch data assets in chronological order starting with the oldest ones. </p>"},{"location":"developers/specification/requirements/#configure-workloads","title":"Configure workloads","text":"<p>Allow to specify the maximum workload that a single job processes. This includes two possbile criteria. First, define the maximum number of data assets. Second, define the maximum weight as the total sum of data asset size. </p> <p>The weight criteria depends on the existence of a weight attribute being present in the metadata table. If not given, no weight can be computed. </p> <p>In case no criteria is specified, all non-processed data is fetched. If both criteria are specified, the one which first exceeds the threshold applies.</p>"},{"location":"developers/specification/requirements/#retry-mechanism","title":"Retry mechanism","text":"<p>Allow to select failed batches of data assets for retry processing. Store the number of attempts for a batch of data assets. Allow a configurable retry count upon failure.</p>"},{"location":"developers/specification/requirements/#single-job-and-multiple-batches","title":"Single job and multiple batches","text":"<p>Allow a single job to process multiple batches consecutively while specifying the total number of iterations. </p>"},{"location":"developers/specification/requirements/#recursivity","title":"Recursivity","text":""},{"location":"developers/specification/requirements/#concurrent-processing","title":"Concurrent processing","text":"<p>Enable multiple jobs to process disjunct ranges of a single type of data assets simultaneously. This allows to parallelize jobs with predictable workloads to quickly process large amounts of backlogs. </p> <p>Before a job starts processing data assets, it has to acquire them. This marks these data assets as being reserved while preventing other jobs to simultaneously process them. Once the job has finished, it can release the of data assets with a given result status such as being succeeded or failed. </p>"},{"location":"developers/specification/requirements/#observability_1","title":"Observability","text":""},{"location":"developers/specification/requirements/#non-functional-requirements","title":"Non-functional requirements","text":""},{"location":"developers/specification/requirements/#usability","title":"Usability","text":""},{"location":"developers/specification/requirements/#reliability","title":"Reliability","text":""},{"location":"developers/specification/requirements/#performance","title":"Performance","text":""},{"location":"developers/specification/requirements/#security","title":"Security","text":""},{"location":"developers/specification/requirements/#maintainability","title":"Maintainability","text":""},{"location":"developers/specification/requirements/#interfaces","title":"Interfaces","text":""},{"location":"developers/specification/requirements/#programmatic-api","title":"Programmatic API","text":""},{"location":"developers/specification/requirements/#command-line-client-cli","title":"Command Line Client CLI","text":""}]}