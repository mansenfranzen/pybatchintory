{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"developer/","text":"Requirements specification This document outlines the requirements specification for a software project named \"pybatchintory\". It focuses on what the software is supposed to accomplish. The problem statement is formulated and the added value is justified. Functional and non functional requirements are derived. This is primarily intended for developers, architects and stakeholders. General description Purpose pybatchintory represents a middleware for batch processing data pipelines. It may greatly improve performance, lower maintenance and enhance observability by enabling the following features: Incremental processing: Process only new, unseen data assets avoiding recomputation. Backfilling: Reprocess historical data assets in a configurable and automated fashion without manual intervention. Predictable workloads: Specify the amount of data assets to be processed to match compute resources for best efficiency. Transparency: Enrich processed data assets with job details like identifier, configuration and timestamp. From a high level perspective, a batch processing application (e.g., Apache Spark or Dask) delegates the generation of batches of data assets to pybatchintory . Conceptionally, pybatchintory only leverages the metadata about data assets and does not read their actual content. Rationale The following section summarizes the contextual background from which pybatchintory originates with its purpose and added value. Incremental processing While incremental processing is supported in stream processing frameworks by design, this functionality is rarely available in batch processing frameworks. Only very few off-the-shelf solutions provide easy-to-use abstractions to handle incremental batch semantics (e.g., AWS Glue Bookmarks). More often than not, custom non-standard solutions are used in production environments. Interestingly, these rely on timestamp watermarks or offsets which closely resemble what stream processing frameworks offer out-of-the-box. Backfilling Reprocessing of data is not an uncommon theme. Bugs need to be fixed in production pipelines and new features have to be computed for historical data, too. However, first class support for such backfilling scenarios is currently not available. In production environments, large volumes of data accumulate over time which eventually become costly and error prone to be processed all at once. Hence, manual planning and execution is often required to create processable chunks of work. Predictable Workloads Data assets are not generated evenly over time. Rather, the data generation process often exhibits seasonality and spikes. If auto-scaling is not available, processing applications may drastically drop in performance (e.g., insufficient memory -> disk spills) or may even fail completely (e.g., out of memory error). Even if auto-scaling is enabled, performance degradation is likely because ideal parallelization is almost never possible in distributed systems. Currently, batch processing frameworks don't provide native support to limit the amount of data to be processed. Observability Frequently, doubts arise regarding data integrity and reliabilty. Customers may question results presented in a dashboard such as missing data. Hence, underlying data pipelines need to be easily inspectable to verify that all available data assets have been processed correctly. Moreover, you may need to identify when a certain data asset was processed by a certain processing application. While the former scenario is widely supported, there exist little to no solutions for the latter one. Terminiology Precondition In order to generate batches of data assets, the existence of a metadata table is mandatory. At minimum, it needs to contain a reference to the data item (e.g, file location) and a primary key as unique row identifier. Since pybatchintory generates chronologically sorted ranges of data items, the primary key needs to chronologically sorted, too. Limitation The batch generation process will only support consecutive data assets in chronologically increasing order. This greatly reduces complextiy and simplifies the implementation. Future versions may loosen this restriction. User Stories As a developer, I want a dedicated solution to support incremental processing for batch pipelines. As a developer, I want a managed solution to reprocess historical data in configurable chunks. As a developer, I want to specify the amount of data that is consumed by a single processing application job. As a stakeholder/developer, I want predictable run times and best performance/cost-ratio. As a user/developer, I want to understand when a given data asset was processed by which job. As a user/developer, I want to retrieve the backlog size of unprocessed data assets for a given job. Specific requirements Terminiology This section extends the existing terminiology with more detailed concepts which are relevant for the specific requirements. Backlog: Describes the remaining amount of unprocessed data assets for a given job. Data asset type: Summarizes data assets that semantically belong together such as a directory of parquet files containing IoT data accross various devices and time ranges. Batch window: Specifies the range of data assets to be included in the batch generation process. Data assets outside this window are not relevant for the batch generation process. The window is defined by a lower and upper boundary which define the first and last data assets to be included, respectively. Batch status: Defines the possible states of a batch of data assets. This includes \"processing\", \"succeeded\" and \"failed\". Functional requirements Acquire & Release Before a job starts processing data assets, it has to acquire them. This marks these data assets as being reserved while preventing other jobs to simultaneously process them. Once the job has finished, it can release the of data assets with a given result status such as being succeeded or failed. Batch Generation This subsection represents the functionality that is required to generate a batch of data assets. Data Asset Selection Allow defining the metadata table that references the relevant data assets. Data Asset Filtering Allow providing a filter condition to subset data assets from the metadata table. This is crucial in case the metadata table holds references to multiple data asset types. The filtering method should be as flexible as possible to support a wide range of metadata table models. Incremental processing Allow fetching non-processed data assets with no upper boundary for the batch window. Hence, newly added data assets will be included from one job run to another. The total amount of relevant data assets is dynamic and changes over time. Backfilling Allow fetching non-processed data assets with an upper boundary for the batch window. Hence, newly added data assets will be excluded from one job run to another. The total amount of relevant data assets is fixed and remains constant over time. Chronology Always fetch data assets in chronological order starting with the oldest ones. Predictable workloads Allow to specify the maximum workload that a single job processes. This includes two possbile criteria. First, define the maximum number of data assets. Second, define the maximum weight as the total sum of data asset size. The weight criteria depends on the existance of a weight attribute being present in the metadata table. If not given, no weight can be computed. In case no criteria is specified, all non-processed data is fetched. If both criteria are specified, the one which first exceeds the threshold applies. Retry mechanism Allow to select failed batches of data assets for retry processing. Store the number of attempts for a batch of data assets. Allow a configurable retry count upon failure. Single job and multiple batches Allow a single job to process multiple batches consecutively while specifying the total number of iterations. Concurrent processing Enable multiple jobs to process disjunct ranges of a single type of data assets simultaneously. This allows to parallize jobs with predictable workloads to quickly process large amounts of backlogs. Observability Non-functional requirements Usability Reliability Performance Security Maintainability Interfaces Programmatic API Command Line Client CLI","title":"Developer"},{"location":"developer/#requirements-specification","text":"This document outlines the requirements specification for a software project named \"pybatchintory\". It focuses on what the software is supposed to accomplish. The problem statement is formulated and the added value is justified. Functional and non functional requirements are derived. This is primarily intended for developers, architects and stakeholders.","title":"Requirements specification"},{"location":"developer/#general-description","text":"","title":"General description"},{"location":"developer/#purpose","text":"pybatchintory represents a middleware for batch processing data pipelines. It may greatly improve performance, lower maintenance and enhance observability by enabling the following features: Incremental processing: Process only new, unseen data assets avoiding recomputation. Backfilling: Reprocess historical data assets in a configurable and automated fashion without manual intervention. Predictable workloads: Specify the amount of data assets to be processed to match compute resources for best efficiency. Transparency: Enrich processed data assets with job details like identifier, configuration and timestamp. From a high level perspective, a batch processing application (e.g., Apache Spark or Dask) delegates the generation of batches of data assets to pybatchintory . Conceptionally, pybatchintory only leverages the metadata about data assets and does not read their actual content.","title":"Purpose"},{"location":"developer/#rationale","text":"The following section summarizes the contextual background from which pybatchintory originates with its purpose and added value.","title":"Rationale"},{"location":"developer/#incremental-processing","text":"While incremental processing is supported in stream processing frameworks by design, this functionality is rarely available in batch processing frameworks. Only very few off-the-shelf solutions provide easy-to-use abstractions to handle incremental batch semantics (e.g., AWS Glue Bookmarks). More often than not, custom non-standard solutions are used in production environments. Interestingly, these rely on timestamp watermarks or offsets which closely resemble what stream processing frameworks offer out-of-the-box.","title":"Incremental processing"},{"location":"developer/#backfilling","text":"Reprocessing of data is not an uncommon theme. Bugs need to be fixed in production pipelines and new features have to be computed for historical data, too. However, first class support for such backfilling scenarios is currently not available. In production environments, large volumes of data accumulate over time which eventually become costly and error prone to be processed all at once. Hence, manual planning and execution is often required to create processable chunks of work.","title":"Backfilling"},{"location":"developer/#predictable-workloads","text":"Data assets are not generated evenly over time. Rather, the data generation process often exhibits seasonality and spikes. If auto-scaling is not available, processing applications may drastically drop in performance (e.g., insufficient memory -> disk spills) or may even fail completely (e.g., out of memory error). Even if auto-scaling is enabled, performance degradation is likely because ideal parallelization is almost never possible in distributed systems. Currently, batch processing frameworks don't provide native support to limit the amount of data to be processed.","title":"Predictable Workloads"},{"location":"developer/#observability","text":"Frequently, doubts arise regarding data integrity and reliabilty. Customers may question results presented in a dashboard such as missing data. Hence, underlying data pipelines need to be easily inspectable to verify that all available data assets have been processed correctly. Moreover, you may need to identify when a certain data asset was processed by a certain processing application. While the former scenario is widely supported, there exist little to no solutions for the latter one.","title":"Observability"},{"location":"developer/#terminiology","text":"","title":"Terminiology"},{"location":"developer/#precondition","text":"In order to generate batches of data assets, the existence of a metadata table is mandatory. At minimum, it needs to contain a reference to the data item (e.g, file location) and a primary key as unique row identifier. Since pybatchintory generates chronologically sorted ranges of data items, the primary key needs to chronologically sorted, too.","title":"Precondition"},{"location":"developer/#limitation","text":"The batch generation process will only support consecutive data assets in chronologically increasing order. This greatly reduces complextiy and simplifies the implementation. Future versions may loosen this restriction.","title":"Limitation"},{"location":"developer/#user-stories","text":"As a developer, I want a dedicated solution to support incremental processing for batch pipelines. As a developer, I want a managed solution to reprocess historical data in configurable chunks. As a developer, I want to specify the amount of data that is consumed by a single processing application job. As a stakeholder/developer, I want predictable run times and best performance/cost-ratio. As a user/developer, I want to understand when a given data asset was processed by which job. As a user/developer, I want to retrieve the backlog size of unprocessed data assets for a given job.","title":"User Stories"},{"location":"developer/#specific-requirements","text":"","title":"Specific requirements"},{"location":"developer/#terminiology_1","text":"This section extends the existing terminiology with more detailed concepts which are relevant for the specific requirements. Backlog: Describes the remaining amount of unprocessed data assets for a given job. Data asset type: Summarizes data assets that semantically belong together such as a directory of parquet files containing IoT data accross various devices and time ranges. Batch window: Specifies the range of data assets to be included in the batch generation process. Data assets outside this window are not relevant for the batch generation process. The window is defined by a lower and upper boundary which define the first and last data assets to be included, respectively. Batch status: Defines the possible states of a batch of data assets. This includes \"processing\", \"succeeded\" and \"failed\".","title":"Terminiology"},{"location":"developer/#functional-requirements","text":"","title":"Functional requirements"},{"location":"developer/#acquire-release","text":"Before a job starts processing data assets, it has to acquire them. This marks these data assets as being reserved while preventing other jobs to simultaneously process them. Once the job has finished, it can release the of data assets with a given result status such as being succeeded or failed.","title":"Acquire &amp; Release"},{"location":"developer/#batch-generation","text":"This subsection represents the functionality that is required to generate a batch of data assets.","title":"Batch Generation"},{"location":"developer/#data-asset-selection","text":"Allow defining the metadata table that references the relevant data assets.","title":"Data Asset Selection"},{"location":"developer/#data-asset-filtering","text":"Allow providing a filter condition to subset data assets from the metadata table. This is crucial in case the metadata table holds references to multiple data asset types. The filtering method should be as flexible as possible to support a wide range of metadata table models.","title":"Data Asset Filtering"},{"location":"developer/#incremental-processing_1","text":"Allow fetching non-processed data assets with no upper boundary for the batch window. Hence, newly added data assets will be included from one job run to another. The total amount of relevant data assets is dynamic and changes over time.","title":"Incremental processing"},{"location":"developer/#backfilling_1","text":"Allow fetching non-processed data assets with an upper boundary for the batch window. Hence, newly added data assets will be excluded from one job run to another. The total amount of relevant data assets is fixed and remains constant over time.","title":"Backfilling"},{"location":"developer/#chronology","text":"Always fetch data assets in chronological order starting with the oldest ones.","title":"Chronology"},{"location":"developer/#predictable-workloads_1","text":"Allow to specify the maximum workload that a single job processes. This includes two possbile criteria. First, define the maximum number of data assets. Second, define the maximum weight as the total sum of data asset size. The weight criteria depends on the existance of a weight attribute being present in the metadata table. If not given, no weight can be computed. In case no criteria is specified, all non-processed data is fetched. If both criteria are specified, the one which first exceeds the threshold applies.","title":"Predictable workloads"},{"location":"developer/#retry-mechanism","text":"Allow to select failed batches of data assets for retry processing. Store the number of attempts for a batch of data assets. Allow a configurable retry count upon failure.","title":"Retry mechanism"},{"location":"developer/#single-job-and-multiple-batches","text":"Allow a single job to process multiple batches consecutively while specifying the total number of iterations.","title":"Single job and multiple batches"},{"location":"developer/#concurrent-processing","text":"Enable multiple jobs to process disjunct ranges of a single type of data assets simultaneously. This allows to parallize jobs with predictable workloads to quickly process large amounts of backlogs.","title":"Concurrent processing"},{"location":"developer/#observability_1","text":"","title":"Observability"},{"location":"developer/#non-functional-requirements","text":"","title":"Non-functional requirements"},{"location":"developer/#usability","text":"","title":"Usability"},{"location":"developer/#reliability","text":"","title":"Reliability"},{"location":"developer/#performance","text":"","title":"Performance"},{"location":"developer/#security","text":"","title":"Security"},{"location":"developer/#maintainability","text":"","title":"Maintainability"},{"location":"developer/#interfaces","text":"","title":"Interfaces"},{"location":"developer/#programmatic-api","text":"","title":"Programmatic API"},{"location":"developer/#command-line-client-cli","text":"","title":"Command Line Client CLI"}]}